{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying an Amazon Comprehend Model with SageMaker Pipelines\n",
    "\n",
    "This example notebook showcases how you can deploy a custom text classification using Amazon Comprehend and SageMaker Pipelines. At the time of writing, there was no native support of AWS Lambda in SageMaker Pipeline. Hence, integration was performed using Amazon SageMaker Processing Jobs.\n",
    "\n",
    "Before you start make sure that your Sagemaker Execution Role has the following policies:\n",
    "\n",
    "- `ComprehendFullAccess`\n",
    "- `AmazonSageMakerFullAccess`\n",
    "- `AWSLambda_FullAccess`\n",
    "\n",
    "Your Sagemaker Execution Role should have access to S3 already. If not you can add the S3 full access policy.\n",
    "You will also need to add `iam:passRole` as an inline policy.\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Finally you will need the following trust policies.\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": [\n",
    "          \"sagemaker.amazonaws.com\",\n",
    "          \"s3.amazonaws.com\",\n",
    "          \"comprehend.amazonaws.com\",\n",
    "          \"lambda.amazonaws.com\"\n",
    "        ]\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "First, we are going to import the SageMaker SDK and set some default variables such as the `role` for permissioned execution and the `default_bucket` to store model artifacts.\n",
    "\n",
    "You will need to use sagemaker 2.x, in case it isn't already upgraded, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role_arn = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aws-ml-blog/artifacts/comprehend-custom-classification/comprehend-train.csv s3://$default_bucket/\n",
    "!aws s3 cp s3://aws-ml-blog/artifacts/comprehend-custom-classification/comprehend-test.csv s3://$default_bucket/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define parameters that can be set for the execution of the pipeline. They serve as variables. We define the following:\n",
    "\n",
    "- `ProcessingInstanceType`: The number of processing instances to use for the execution of the pipeline\n",
    "- `TrainData`: Location of the training data in S3\n",
    "- `TestData`: Location of the test data in S3\n",
    "- `RoleArn`: ARN (Amazon Resource Name) of the role used for pipeline execution\n",
    "- `ModelOutput`: Location of the target S3 path for the Amazon Comprehend model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "input_train = ParameterString(\n",
    "    name=\"TrainData\",\n",
    "    default_value=f\"s3://{default_bucket}/comprehend-train.csv\",\n",
    ")\n",
    "\n",
    "input_test = ParameterString(\n",
    "    name=\"TestData\",\n",
    "    default_value=f\"s3://{default_bucket}/comprehend-test.csv\",\n",
    ")\n",
    "\n",
    "iam_role_arn = ParameterString(\n",
    "    name=\"RoleArn\",\n",
    "    default_value=role_arn,\n",
    ")\n",
    "\n",
    "model_output = ParameterString(\n",
    "    name=\"ModelOutput\",\n",
    "    default_value=f\"s3://{default_bucket}/model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#sagemaker.sklearn.processing.SKLearnProcessor) to run Python scripts to train, and deploy Amazon Comprehend models using `boto3`. In the next chunk, we instantiate an instance of `SKLearnProcessor` that we use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"comprehend-process\",\n",
    "    role=role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Amazon SageMaker [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=ProcessingStep#sagemaker.workflow.steps.ProcessingStep) provides a containerized execution environment to run the `prepare_data.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ProcessingStep(\n",
    "    name=\"ComprehendProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_train, destination=\"/opt/ml/processing/input_train\"),\n",
    "        ProcessingInput(source=input_test, destination=\"/opt/ml/processing/input_test\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"prepare_data.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second Amazon SageMaker processing step trains the Amazon Comprehend model by running `train_eval_comprehend.py`. Amazon Comprehend automatically evaluates the performance on an evaluation set. We will use that score as a condition for deploying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report = PropertyFile(\n",
    "        name=\"ComprehendEvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_train_and_eval = ProcessingStep(\n",
    "    name=\"ComprehendTrainAndEval\",\n",
    "    processor=sklearn_processor,\n",
    "    job_arguments=['--train-input-file', preprocess.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                   '--train-output-path', model_output,\n",
    "                   '--iam-role-arn', role_arn\n",
    "                  ],\n",
    "    code=\"train_eval_comprehend.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "        ProcessingOutput(output_name=\"arn\", source=\"/opt/ml/processing/arn\")\n",
    "    ],\n",
    "    property_files=[evaluation_report]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third Amazon SageMaker processing step deploys the Amazon Comprehend model running `deploy_comprehend.py`. If the Accuracy reported after training is lower than a certain threshold, this step does not run and the pipeline stops here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_deploy_model = ProcessingStep(\n",
    "    name=\"ComprehendDeploy\",\n",
    "    processor=sklearn_processor,\n",
    "    job_arguments=['--arn-path', comprehend_train_and_eval.properties.ProcessingOutputConfig.Outputs[\"arn\"].S3Output.S3Uri],\n",
    "    code=\"deploy_comprehend.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"endpoint_arn\", source=\"/opt/ml/processing/endpoint_arn\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_lte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name='ComprehendTrainAndEval',\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"Accuracy\"\n",
    "    ),\n",
    "    right=0.65\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"ComprehendAccuracyCondition\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_deploy_model],\n",
    "    else_steps=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the deployed model can be used for inference. At this stage we use [AWS Lambda](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.lambda_step.LambdaStep) to call the Amazon Comprehend endpoint with the text of our choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Italian EBU Member RAI has won the 65th Eurovision Song Contest with the song\" + \\\n",
    "\"Zitti e buoni performed by Måneskin. It's the 3rd win for Italy who last triumphed in 1990. \" + \\\n",
    "\"26 countries took part in the Grand Final of the world’s largest live music event, \" + \\\n",
    "\"hosted by Dutch EBU Members NPO, NOS and AVROTROS on Saturday 22 May in Rotterdam. \" + \\\n",
    "\"Måneskin wrote the winning song which finished the night with 524 points, 25 points \" + \\\n",
    "\"ahead of 2nd placed France represented by Barbara Pravi singing Voila. Switzerland’s Gjon’s Tears with Tout l’Univers finished in third place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Lambda Step\n",
    "function_name = \"sagemaker-lambda-step-endpoint-test\"\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=role_arn,\n",
    "    script=\"test_comprehend_lambda.py\",\n",
    "    handler=\"test_comprehend_lambda.lambda_handler\",\n",
    ")\n",
    "\n",
    "test_endpoint = LambdaStep(\n",
    "    name=\"LambdaStep\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"endpoint_arn_path\": step_deploy_model.properties.ProcessingOutputConfig.Outputs[\"endpoint_arn\"].S3Output.S3Uri,\n",
    "        \"text\": example_text,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"ComprehendPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        input_train,\n",
    "        input_test,\n",
    "        iam_role_arn,\n",
    "        model_output\n",
    "    ],\n",
    "    steps=[preprocess, comprehend_train_and_eval, step_cond, test_endpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
